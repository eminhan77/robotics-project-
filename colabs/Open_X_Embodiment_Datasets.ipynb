{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnWCKLGGaf-d"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcHkjVaXJzHe"
      },
      "source": [
        "Copyright 2020 DeepMind Technologies Limited.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2o2t0HVJzHf"
      },
      "source": [
        "# Open X-Embodiment Datasets\n",
        "\n",
        "![](https://robotics-transformer-x.github.io/img/overview.png)\n",
        "\n",
        "This colab helps you **visualize** the datasets in the Open X-Embodiment Dataset, explains how to **download** them and how to **train** with them.\n",
        "\n",
        "Table of Content:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "toc",
        "id": "UyiiBjzmaIQu"
      },
      "source": [
        ">[Open X-Embodiment Datasets](#scrollTo=EnWCKLGGaf-d)\n",
        "\n",
        ">[Visualize Datasets](#scrollTo=29c7oLlJbWwF)\n",
        "\n",
        ">[Download Datasets](#scrollTo=-WHN-2OrKqGo)\n",
        "\n",
        ">[Data Loader Example](#scrollTo=IyccDsRqwtMz)\n",
        "\n",
        ">[Interleave Multiple Datasets](#scrollTo=ekmsGRAnw3Bp)\n",
        "\n",
        ">[Example Dataloader to produce trajectories](#scrollTo=aew258oUbamg)\n",
        "\n",
        ">>[Demonstration of transformation from an episode to a trajectory](#scrollTo=BK4RRYkbLN5B)\n",
        "\n",
        ">>[Combination of multiple datasets](#scrollTo=Oy89HzymQyAq)\n",
        "\n",
        ">[Available datasets:](#scrollTo=N2Efw2aHVfSX)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29c7oLlJbWwF"
      },
      "source": [
        "# Visualize Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l7OogZYi7qwT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "from PIL import Image\n",
        "from IPython import display\n",
        "\n",
        "DATASETS = [\n",
        "    'fractal20220817_data',\n",
        "    'kuka',\n",
        "    'bridge',\n",
        "    'taco_play',\n",
        "    'jaco_play',\n",
        "    'berkeley_cable_routing',\n",
        "    'roboturk',\n",
        "    'nyu_door_opening_surprising_effectiveness',\n",
        "    'viola',\n",
        "    'berkeley_autolab_ur5',\n",
        "    'toto',\n",
        "    'language_table',\n",
        "    'columbia_cairlab_pusht_real',\n",
        "    'stanford_kuka_multimodal_dataset_converted_externally_to_rlds',\n",
        "    'nyu_rot_dataset_converted_externally_to_rlds',\n",
        "    'stanford_hydra_dataset_converted_externally_to_rlds',\n",
        "    'austin_buds_dataset_converted_externally_to_rlds',\n",
        "    'nyu_franka_play_dataset_converted_externally_to_rlds',\n",
        "    'maniskill_dataset_converted_externally_to_rlds',\n",
        "    'cmu_franka_exploration_dataset_converted_externally_to_rlds',\n",
        "    'ucsd_kitchen_dataset_converted_externally_to_rlds',\n",
        "    'ucsd_pick_and_place_dataset_converted_externally_to_rlds',\n",
        "    'austin_sailor_dataset_converted_externally_to_rlds',\n",
        "    'austin_sirius_dataset_converted_externally_to_rlds',\n",
        "    'bc_z',\n",
        "    'usc_cloth_sim_converted_externally_to_rlds',\n",
        "    'utokyo_pr2_opening_fridge_converted_externally_to_rlds',\n",
        "    'utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds',\n",
        "    'utokyo_saytap_converted_externally_to_rlds',\n",
        "    'utokyo_xarm_pick_and_place_converted_externally_to_rlds',\n",
        "    'utokyo_xarm_bimanual_converted_externally_to_rlds',\n",
        "    'robo_net',\n",
        "    'berkeley_mvp_converted_externally_to_rlds',\n",
        "    'berkeley_rpt_converted_externally_to_rlds',\n",
        "    'kaist_nonprehensile_converted_externally_to_rlds',\n",
        "    'stanford_mask_vit_converted_externally_to_rlds',\n",
        "    'tokyo_u_lsmo_converted_externally_to_rlds',\n",
        "    'dlr_sara_pour_converted_externally_to_rlds',\n",
        "    'dlr_sara_grid_clamp_converted_externally_to_rlds',\n",
        "    'dlr_edan_shared_control_converted_externally_to_rlds',\n",
        "    'asu_table_top_converted_externally_to_rlds',\n",
        "    'stanford_robocook_converted_externally_to_rlds',\n",
        "    'eth_agent_affordances',\n",
        "    'imperialcollege_sawyer_wrist_cam',\n",
        "    'iamlab_cmu_pickup_insert_converted_externally_to_rlds',\n",
        "    'uiuc_d3field',\n",
        "    'utaustin_mutex',\n",
        "    'berkeley_fanuc_manipulation',\n",
        "    'cmu_play_fusion',\n",
        "    'cmu_stretch',\n",
        "    'berkeley_gnm_recon',\n",
        "    'berkeley_gnm_cory_hall',\n",
        "    'berkeley_gnm_sac_son'\n",
        "]\n",
        "\n",
        "\n",
        "def dataset2path(dataset_name):\n",
        "  if dataset_name == 'robo_net':\n",
        "    version = '1.0.0'\n",
        "  elif dataset_name == 'language_table':\n",
        "    version = '0.0.1'\n",
        "  else:\n",
        "    version = '0.1.0'\n",
        "  return f'gs://gresearch/robotics/{dataset_name}/{version}'\n",
        "\n",
        "\n",
        "def as_gif(images, path='temp.gif'):\n",
        "  # Render the images as the gif:\n",
        "  images[0].save(path, save_all=True, append_images=images[1:], duration=1000, loop=0)\n",
        "  gif_bytes = open(path,'rb').read()\n",
        "  return gif_bytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y-powK6wJzHi",
        "outputId": "2e0466a2-ff37-4c79-85e9-d4183b060ead",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.25.2\n",
            "  Downloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.25.2 which is incompatible.\n",
            "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.25.2 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.25.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.25.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "f1836737600141e0b13b4083be5a0861"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install numpy==1.25.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "Gcw4eHmxbZjx",
        "outputId": "4f5c7b53-eb51-44de-9650-01efed61f7fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _xla_gc_callback at 0x7e5a80a03ba0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py\", line 96, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "    \n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5f53add848cd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdisplay_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'image'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilder_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset2path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdisplay_key\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'steps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'observation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_datasets/core/read_only_builder.py\u001b[0m in \u001b[0;36mbuilder_from_directory\u001b[0;34m(builder_dir, file_format)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mbuilder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetBuilder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuilder\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0mat\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m   \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mReadOnlyBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilder_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuilder_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_datasets/core/logging/__init__.py\u001b[0m in \u001b[0;36mdecorator\u001b[0;34m(function, dsbuilder, args, kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0m_thread_id_to_builder_init_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthread_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_datasets/core/read_only_builder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, builder_dir, info_proto, file_format)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mbuilder_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilder_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minfo_proto\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m       \u001b[0minfo_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_proto_from_builder_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilder_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo_proto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_datasets/core/dataset_info.py\u001b[0m in \u001b[0;36mread_proto_from_builder_dir\u001b[0;34m(builder_dir)\u001b[0m\n\u001b[1;32m   1156\u001b[0m   \u001b[0mbuilder_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilder_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m   \u001b[0minfo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATASET_INFO_FILENAME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_datasets/core/dataset_info.py\u001b[0m in \u001b[0;36mread_from_json\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m   1132\u001b[0m   \"\"\"\n\u001b[1;32m   1133\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m     \u001b[0mjson_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1135\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Could not load dataset info from {path}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/etils/epath/abstract_path.py\u001b[0m in \u001b[0;36mread_text\u001b[0;34m(self, encoding)\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mread_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;34m\"\"\"Reads contents of self as a string.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/etils/epath/backend.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m       \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m   ) -> Iterator[typing.IO[Union[str, bytes]]]:\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pytype: disable=bad-return-type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/etils/epath/backend.py\u001b[0m in \u001b[0;36mgfile\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/etils/epath/backend.py\u001b[0m in \u001b[0;36mtf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m       \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top  # pytype: disable=import-error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m       raise ImportError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf_keras.src.optimizers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m     \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"keras.src.optimizers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m   \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# DO NOT EDIT. Generated by api_gen.sh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDTypePolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFloatDTypePolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInitializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/api/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/api/activations/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/activations/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_export\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mobject_registration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserialization_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_registration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_registered_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_registration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_keras_serializable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialization_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialization_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_export\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlegacy_h5_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msaving_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfile_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msaving_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mobject_registration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/saving_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserialization\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlegacy_serialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_build_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_mapping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_saveable_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msaving_api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbase_trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraceback_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompileLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompileMetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_adapters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_adapter_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpython_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraceback_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribution_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_adapters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_data_adapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_adapters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_adapters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpy_dataset_adapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/array_data_adapter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_adapters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_adapters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_adapter_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_adapters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_adapter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataAdapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/array_slicing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mpandas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;31m# These were moved in 1.25 and may be deprecated eventually:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;34m\"ModuleDeprecationWarning\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"VisibleDeprecationWarning\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0;34m\"ComplexWarning\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TooHardError\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AxisError\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         }\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpublic_symbols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "# choose the dataset path in the dropdown on the right and rerun this cell\n",
        "# to see multiple samples\n",
        "\n",
        "dataset = 'fractal20220817_data' # @param ['fractal20220817_data', 'kuka', 'bridge', 'taco_play', 'jaco_play', 'berkeley_cable_routing', 'roboturk', 'nyu_door_opening_surprising_effectiveness', 'viola', 'berkeley_autolab_ur5', 'toto', 'language_table', 'columbia_cairlab_pusht_real', 'stanford_kuka_multimodal_dataset_converted_externally_to_rlds', 'nyu_rot_dataset_converted_externally_to_rlds', 'stanford_hydra_dataset_converted_externally_to_rlds', 'austin_buds_dataset_converted_externally_to_rlds', 'nyu_franka_play_dataset_converted_externally_to_rlds', 'maniskill_dataset_converted_externally_to_rlds', 'furniture_bench_dataset_converted_externally_to_rlds', 'cmu_franka_exploration_dataset_converted_externally_to_rlds', 'ucsd_kitchen_dataset_converted_externally_to_rlds', 'ucsd_pick_and_place_dataset_converted_externally_to_rlds', 'austin_sailor_dataset_converted_externally_to_rlds', 'austin_sirius_dataset_converted_externally_to_rlds', 'bc_z', 'usc_cloth_sim_converted_externally_to_rlds', 'utokyo_pr2_opening_fridge_converted_externally_to_rlds', 'utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds', 'utokyo_saytap_converted_externally_to_rlds', 'utokyo_xarm_pick_and_place_converted_externally_to_rlds', 'utokyo_xarm_bimanual_converted_externally_to_rlds', 'robo_net', 'berkeley_mvp_converted_externally_to_rlds', 'berkeley_rpt_converted_externally_to_rlds', 'kaist_nonprehensile_converted_externally_to_rlds', 'stanford_mask_vit_converted_externally_to_rlds', 'tokyo_u_lsmo_converted_externally_to_rlds', 'dlr_sara_pour_converted_externally_to_rlds', 'dlr_sara_grid_clamp_converted_externally_to_rlds', 'dlr_edan_shared_control_converted_externally_to_rlds', 'asu_table_top_converted_externally_to_rlds', 'stanford_robocook_converted_externally_to_rlds', 'eth_agent_affordances', 'imperialcollege_sawyer_wrist_cam', 'iamlab_cmu_pickup_insert_converted_externally_to_rlds', 'uiuc_d3field', 'utaustin_mutex', 'berkeley_fanuc_manipulation', 'cmu_food_manipulation', 'cmu_play_fusion', 'cmu_stretch', 'berkeley_gnm_recon', 'berkeley_gnm_cory_hall', 'berkeley_gnm_sac_son']\n",
        "display_key = 'image'\n",
        "\n",
        "b = tfds.builder_from_directory(builder_dir=dataset2path(dataset))\n",
        "if display_key not in b.info.features['steps']['observation']:\n",
        "  raise ValueError(\n",
        "      f\"The key {display_key} was not found in this dataset.\\n\"\n",
        "      + \"Please choose a different image key to display for this dataset.\\n\"\n",
        "      + \"Here is the observation spec:\\n\"\n",
        "      + str(b.info.features['steps']['observation']))\n",
        "\n",
        "ds = b.as_dataset(split='train[:10]').shuffle(10)   # take only first 10 episodes\n",
        "episode = next(iter(ds))\n",
        "images = [step['observation'][display_key] for step in episode['steps']]\n",
        "images = [Image.fromarray(image.numpy()) for image in images]\n",
        "display.Image(as_gif(images))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrD4_8P9JxBw"
      },
      "outputs": [],
      "source": [
        "# other elements of the episode step --> this may vary for each dataset\n",
        "for elem in next(iter(episode['steps'])).items():\n",
        "  print(elem)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WHN-2OrKqGo"
      },
      "source": [
        "# Download Datasets\n",
        "\n",
        "All datasets can be downloaded simply via `tfds.load(<dataset_name>)`.\n",
        "Below we provide a script that downloads all datasets into `~/tensorflow_datasets` on your local machine. Simply copy the code and run it on your local machine to download the full dataset (XXX TB).\n",
        "\n",
        "If you want to filter the dataset before download, please refer to\n",
        "[this Google Sheet](https://docs.google.com/spreadsheets/d/1rPBD77tk60AEIGZrGSODwyyzs5FgCU9Uz3h-3_t2A9g/edit?usp=sharing). It allows you\n",
        "to filter the data by attributes like robot model, number of cameras, type of tasks etc. You can then download only the filtered datasets by pasting the\n",
        "dataset list from the spreadsheet into the code below.\n",
        "\n",
        "The download code will automatically skip any datasets you have previously downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcsQuLjY7c0o"
      },
      "outputs": [],
      "source": [
        "!pip install tfds-nightly   # to get most up-to-date registered datasets\n",
        "!pip install apache_beam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EjbapLxJzHk"
      },
      "source": [
        "warning: the below cell serves as an example how to download the datasets to your local directory. It usually takes a long time to run and require a relatively large disk size available, you could skip it or end it if you don't need to download the datasets. To speedup, please have a look at the instructions to setup distributed generation:\n",
        "https://www.tensorflow.org/datasets/beam_datasets#generating_a_beam_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtNplr0AP-ZH"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tqdm\n",
        "\n",
        "# optionally replace the DATASET_NAMES below with the list of filtered datasets from the google sheet\n",
        "DATASET_NAMES = ['fractal20220817_data', 'kuka', 'bridge', 'taco_play', 'jaco_play', 'berkeley_cable_routing', 'roboturk', 'nyu_door_opening_surprising_effectiveness', 'viola', 'berkeley_autolab_ur5', 'toto', 'language_table', 'columbia_cairlab_pusht_real', 'stanford_kuka_multimodal_dataset_converted_externally_to_rlds', 'nyu_rot_dataset_converted_externally_to_rlds', 'stanford_hydra_dataset_converted_externally_to_rlds', 'austin_buds_dataset_converted_externally_to_rlds', 'nyu_franka_play_dataset_converted_externally_to_rlds', 'maniskill_dataset_converted_externally_to_rlds', 'furniture_bench_dataset_converted_externally_to_rlds', 'cmu_franka_exploration_dataset_converted_externally_to_rlds', 'ucsd_kitchen_dataset_converted_externally_to_rlds', 'ucsd_pick_and_place_dataset_converted_externally_to_rlds', 'austin_sailor_dataset_converted_externally_to_rlds', 'austin_sirius_dataset_converted_externally_to_rlds', 'bc_z', 'usc_cloth_sim_converted_externally_to_rlds', 'utokyo_pr2_opening_fridge_converted_externally_to_rlds', 'utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds', 'utokyo_saytap_converted_externally_to_rlds', 'utokyo_xarm_pick_and_place_converted_externally_to_rlds', 'utokyo_xarm_bimanual_converted_externally_to_rlds', 'robo_net', 'berkeley_mvp_converted_externally_to_rlds', 'berkeley_rpt_converted_externally_to_rlds', 'kaist_nonprehensile_converted_externally_to_rlds', 'stanford_mask_vit_converted_externally_to_rlds', 'tokyo_u_lsmo_converted_externally_to_rlds', 'dlr_sara_pour_converted_externally_to_rlds', 'dlr_sara_grid_clamp_converted_externally_to_rlds', 'dlr_edan_shared_control_converted_externally_to_rlds', 'asu_table_top_converted_externally_to_rlds', 'stanford_robocook_converted_externally_to_rlds', 'eth_agent_affordances', 'imperialcollege_sawyer_wrist_cam', 'iamlab_cmu_pickup_insert_converted_externally_to_rlds', 'uiuc_d3field', 'utaustin_mutex', 'berkeley_fanuc_manipulation', 'cmu_food_manipulation', 'cmu_play_fusion', 'cmu_stretch', 'berkeley_gnm_recon', 'berkeley_gnm_cory_hall', 'berkeley_gnm_sac_son']\n",
        "DOWNLOAD_DIR = '~/tensorflow_datasets'\n",
        "\n",
        "print(f\"Downloading {len(DATASET_NAMES)} datasets to {DOWNLOAD_DIR}.\")\n",
        "for dataset_name in tqdm.tqdm(DATASET_NAMES):\n",
        "  _ = tfds.load(dataset_name, data_dir=DOWNLOAD_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyccDsRqwtMz"
      },
      "source": [
        "# Data Loader Example\n",
        "\n",
        "Below, we demonstrate a simple example of how to load the dataset into training batches, where each sample in the batch only contains one step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X17VECdRwzka"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# load raw dataset --> replace this with tfds.load(<dataset_name>) on your\n",
        "# local machine!\n",
        "dataset = 'kuka'\n",
        "b = tfds.builder_from_directory(builder_dir=dataset2path(dataset))\n",
        "ds = b.as_dataset(split='train[:10]')\n",
        "\n",
        "def episode2steps(episode):\n",
        "  return episode['steps']\n",
        "\n",
        "def step_map_fn(step):\n",
        "  return {\n",
        "      'observation': {\n",
        "          'image': tf.image.resize(step['observation']['image'], (128, 128)),\n",
        "      },\n",
        "      'action': tf.concat([\n",
        "          step['action']['world_vector'],\n",
        "          step['action']['rotation_delta'],\n",
        "          step['action']['gripper_closedness_action'],\n",
        "      ], axis=-1)\n",
        "  }\n",
        "\n",
        "# convert RLDS episode dataset to individual steps & reformat\n",
        "ds = ds.map(\n",
        "    episode2steps, num_parallel_calls=tf.data.AUTOTUNE).flat_map(lambda x: x)\n",
        "ds = ds.map(step_map_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# shuffle, repeat, pre-fetch, batch\n",
        "ds = ds.cache()         # optionally keep full dataset in memory\n",
        "ds = ds.shuffle(100)    # set shuffle buffer size\n",
        "ds = ds.repeat()        # ensure that data never runs out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0uJH3X6w1LZ"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "\n",
        "for i, batch in tqdm.tqdm(\n",
        "    enumerate(ds.prefetch(3).batch(4).as_numpy_iterator())):\n",
        "  # here you would add your Jax / PyTorch training code\n",
        "  if i == 10000: break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekmsGRAnw3Bp"
      },
      "source": [
        "# Interleave Multiple Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CslwEuBZwmLP"
      },
      "outputs": [],
      "source": [
        "# Load second dataset --> replace this with tfds.load(<dataset_name>) on your\n",
        "# local machine!\n",
        "dataset = 'utaustin_mutex'\n",
        "b = tfds.builder_from_directory(builder_dir=dataset2path(dataset))\n",
        "ds2 = b.as_dataset(split='train[:10]')\n",
        "\n",
        "def step_map_fn_mutex(step):\n",
        "  # reformat to align specs of both datasets\n",
        "  return {\n",
        "      'observation': {\n",
        "          'image': tf.image.resize(step['observation']['image'], (128, 128)),\n",
        "      },\n",
        "      'action': step['action'],\n",
        "  }\n",
        "\n",
        "ds2 = ds2.map(\n",
        "    episode2steps, num_parallel_calls=tf.data.AUTOTUNE).flat_map(lambda x: x)\n",
        "ds2 = ds2.map(step_map_fn_mutex, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# shuffle, repeat, pre-fetch, batch\n",
        "ds2 = ds2.cache()         # optionally keep full dataset in memory\n",
        "ds2 = ds2.shuffle(100)    # set shuffle buffer size\n",
        "ds2 = ds2.repeat()        # ensure that data never runs out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2hcCJd8w6-D"
      },
      "outputs": [],
      "source": [
        "# interleave datasets w/ equal sampling weight\n",
        "ds_combined = tf.data.Dataset.sample_from_datasets([ds, ds2], [0.5, 0.5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEnVFP9nw8iI"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "\n",
        "for i, batch in tqdm.tqdm(\n",
        "    enumerate(ds_combined.prefetch(3).batch(4).as_numpy_iterator())):\n",
        "  # here you would add your Jax / PyTorch training code\n",
        "  if i == 10000: break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aew258oUbamg"
      },
      "source": [
        "# Example Dataloader to produce trajectories\n",
        "\n",
        "When training transformers, we usually use trajectories of fix-length as input into the transformers. This is to enable the transformer to condition on a fixed window of history when predicting actions.\n",
        "\n",
        "Below we demonstrate how one can load the TFDS datasets, transform the episodes\n",
        "into fixed-length \"trajectories\" and mix multiple datasets by aligning their specs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YU0qKdrp7oBT"
      },
      "outputs": [],
      "source": [
        "!pip install rlds[tensorflow]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ds0lu3vJzHl"
      },
      "outputs": [],
      "source": [
        "!pip install dm-reverb[tensorflow]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3b5BEt1JvQJ"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Dict, Union, NamedTuple\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "import rlds\n",
        "import reverb\n",
        "from rlds import transformations\n",
        "import tensorflow_datasets as tfds\n",
        "import tree\n",
        "\n",
        "import abc\n",
        "import dataclasses\n",
        "from typing import Dict, Optional\n",
        "\n",
        "from rlds import rlds_types\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Dgf1OxIhJwib"
      },
      "outputs": [],
      "source": [
        "# @title Transformation definitions\n",
        "\n",
        "def _features_to_tensor_spec(\n",
        "    feature: tfds.features.FeatureConnector\n",
        ") -> tf.TensorSpec:\n",
        "  \"\"\"Converts a tfds Feature into a TensorSpec.\"\"\"\n",
        "\n",
        "  def _get_feature_spec(nested_feature: tfds.features.FeatureConnector):\n",
        "    if isinstance(nested_feature, tf.DType):\n",
        "      return tf.TensorSpec(shape=(), dtype=nested_feature)\n",
        "    else:\n",
        "      return nested_feature.get_tensor_spec()\n",
        "\n",
        "  # FeaturesDict can sometimes be a plain dictionary, so we use tf.nest to\n",
        "  # make sure we deal with the nested structure.\n",
        "  return tf.nest.map_structure(_get_feature_spec, feature)\n",
        "\n",
        "\n",
        "def _encoded_feature(feature: Optional[tfds.features.FeatureConnector],\n",
        "                     image_encoding: Optional[str],\n",
        "                     tensor_encoding: Optional[tfds.features.Encoding]):\n",
        "  \"\"\"Adds encoding to Images and/or Tensors.\"\"\"\n",
        "  def _apply_encoding(feature: tfds.features.FeatureConnector,\n",
        "                      image_encoding: Optional[str],\n",
        "                      tensor_encoding: Optional[tfds.features.Encoding]):\n",
        "    if image_encoding and isinstance(feature, tfds.features.Image):\n",
        "      return tfds.features.Image(\n",
        "          shape=feature.shape,\n",
        "          dtype=feature.dtype,\n",
        "          use_colormap=feature.use_colormap,\n",
        "          encoding_format=image_encoding)\n",
        "    if tensor_encoding and isinstance(\n",
        "        feature, tfds.features.Tensor) and feature.dtype != tf.string:\n",
        "      return tfds.features.Tensor(\n",
        "          shape=feature.shape, dtype=feature.dtype, encoding=tensor_encoding)\n",
        "    return feature\n",
        "\n",
        "  if not feature:\n",
        "    return None\n",
        "  return tf.nest.map_structure(\n",
        "      lambda x: _apply_encoding(x, image_encoding, tensor_encoding), feature)\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class RLDSSpec(metaclass=abc.ABCMeta):\n",
        "  \"\"\"Specification of an RLDS Dataset.\n",
        "\n",
        "  It is used to hold a spec that can be converted into a TFDS DatasetInfo or\n",
        "  a `tf.data.Dataset` spec.\n",
        "  \"\"\"\n",
        "  observation_info: Optional[tfds.features.FeatureConnector] = None\n",
        "  action_info: Optional[tfds.features.FeatureConnector] = None\n",
        "  reward_info: Optional[tfds.features.FeatureConnector] = None\n",
        "  discount_info: Optional[tfds.features.FeatureConnector] = None\n",
        "  step_metadata_info: Optional[tfds.features.FeaturesDict] = None\n",
        "  episode_metadata_info: Optional[tfds.features.FeaturesDict] = None\n",
        "\n",
        "  def step_tensor_spec(self) -> Dict[str, tf.TensorSpec]:\n",
        "    \"\"\"Obtains the TensorSpec of an RLDS step.\"\"\"\n",
        "    step = {}\n",
        "    if self.observation_info:\n",
        "      step[rlds_types.OBSERVATION] = _features_to_tensor_spec(\n",
        "          self.observation_info)\n",
        "    if self.action_info:\n",
        "      step[rlds_types.ACTION] = _features_to_tensor_spec(\n",
        "          self.action_info)\n",
        "    if self.discount_info:\n",
        "      step[rlds_types.DISCOUNT] = _features_to_tensor_spec(\n",
        "          self.discount_info)\n",
        "    if self.reward_info:\n",
        "      step[rlds_types.REWARD] = _features_to_tensor_spec(\n",
        "          self.reward_info)\n",
        "    if self.step_metadata_info:\n",
        "      for k, v in self.step_metadata_info.items():\n",
        "        step[k] = _features_to_tensor_spec(v)\n",
        "\n",
        "    step[rlds_types.IS_FIRST] = tf.TensorSpec(shape=(), dtype=bool)\n",
        "    step[rlds_types.IS_LAST] = tf.TensorSpec(shape=(), dtype=bool)\n",
        "    step[rlds_types.IS_TERMINAL] = tf.TensorSpec(shape=(), dtype=bool)\n",
        "    return step\n",
        "\n",
        "  def episode_tensor_spec(self) -> Dict[str, tf.TensorSpec]:\n",
        "    \"\"\"Obtains the TensorSpec of an RLDS step.\"\"\"\n",
        "    episode = {}\n",
        "    episode[rlds_types.STEPS] = tf.data.DatasetSpec(\n",
        "        element_spec=self.step_tensor_spec())\n",
        "    if self.episode_metadata_info:\n",
        "      for k, v in self.episode_metadata_info.items():\n",
        "        episode[k] = _features_to_tensor_spec(v)\n",
        "    return episode\n",
        "\n",
        "  def to_dataset_config(\n",
        "      self,\n",
        "      name: str,\n",
        "      image_encoding: Optional[str] = None,\n",
        "      tensor_encoding: Optional[tfds.features.Encoding] = None,\n",
        "      citation: Optional[str] = None,\n",
        "      homepage: Optional[str] = None,\n",
        "      description: Optional[str] = None,\n",
        "      overall_description: Optional[str] = None,\n",
        "  ) -> tfds.rlds.rlds_base.DatasetConfig:\n",
        "    \"\"\"Obtains the DatasetConfig for TFDS from the Spec.\"\"\"\n",
        "    return tfds.rlds.rlds_base.DatasetConfig(\n",
        "        name=name,\n",
        "        description=description,\n",
        "        overall_description=overall_description,\n",
        "        homepage=homepage,\n",
        "        citation=citation,\n",
        "        observation_info=_encoded_feature(self.observation_info, image_encoding,\n",
        "                                          tensor_encoding),\n",
        "        action_info=_encoded_feature(self.action_info, image_encoding,\n",
        "                                     tensor_encoding),\n",
        "        reward_info=_encoded_feature(self.reward_info, image_encoding,\n",
        "                                     tensor_encoding),\n",
        "        discount_info=_encoded_feature(self.discount_info, image_encoding,\n",
        "                                       tensor_encoding),\n",
        "        step_metadata_info=_encoded_feature(self.step_metadata_info,\n",
        "                                            image_encoding, tensor_encoding),\n",
        "        episode_metadata_info=_encoded_feature(self.episode_metadata_info,\n",
        "                                               image_encoding, tensor_encoding))\n",
        "\n",
        "  def to_features_dict(self):\n",
        "    \"\"\"Returns a TFDS FeaturesDict representing the dataset config.\"\"\"\n",
        "    step_config = {\n",
        "        rlds_types.IS_FIRST: tf.bool,\n",
        "        rlds_types.IS_LAST: tf.bool,\n",
        "        rlds_types.IS_TERMINAL: tf.bool,\n",
        "    }\n",
        "\n",
        "    if self.observation_info:\n",
        "      step_config[rlds_types.OBSERVATION] = self.observation_info\n",
        "    if self.action_info:\n",
        "      step_config[rlds_types.ACTION] = self.action_info\n",
        "    if self.discount_info:\n",
        "      step_config[rlds_types.DISCOUNT] = self.discount_info\n",
        "    if self.reward_info:\n",
        "      step_config[rlds_types.REWARD] = self.reward_info\n",
        "\n",
        "    if self.step_metadata_info:\n",
        "      for k, v in self.step_metadata_info.items():\n",
        "        step_config[k] = v\n",
        "\n",
        "    if self.episode_metadata_info:\n",
        "      return tfds.features.FeaturesDict({\n",
        "          rlds_types.STEPS: tfds.features.Dataset(step_config),\n",
        "          **self.episode_metadata_info,\n",
        "      })\n",
        "    else:\n",
        "      return tfds.features.FeaturesDict({\n",
        "          rlds_types.STEPS: tfds.features.Dataset(step_config),\n",
        "      })\n",
        "\n",
        "RLDS_SPEC = RLDSSpec\n",
        "TENSOR_SPEC = Union[tf.TensorSpec, dict[str, tf.TensorSpec]]\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class TrajectoryTransform(metaclass=abc.ABCMeta):\n",
        "  \"\"\"Specification the TrajectoryTransform applied to a dataset of episodes.\n",
        "\n",
        "  A TrajectoryTransform is a set of rules transforming a dataset\n",
        "  of RLDS episodes to a dataset of trajectories.\n",
        "  This involves three distinct stages:\n",
        "  - An optional `episode_to_steps_map_fn(episode)` is called at the episode\n",
        "    level, and can be used to select or modify steps.\n",
        "    - Augmentation: an `episode_key` could be propagated to `steps` for\n",
        "      debugging.\n",
        "    - Selection: Particular steps can be selected.\n",
        "    - Stripping: Features can be removed from steps. Prefer using `step_map_fn`.\n",
        "  - An optional `step_map_fn` is called at the flattened steps dataset for each\n",
        "    step, and can be used to featurize a step, e.g. add/remove features, or\n",
        "    augument images\n",
        "  - A `pattern` leverages DM patterns to set a rule of slicing an episode to a\n",
        "    dataset of overlapping trajectories.\n",
        "\n",
        "  Importantly, each TrajectoryTransform must define a `expected_tensor_spec`\n",
        "  which specifies a nested TensorSpec of the resulting dataset. This is what\n",
        "  this TrajectoryTransform will produce, and can be used as an interface with\n",
        "  a neural network.\n",
        "  \"\"\"\n",
        "  episode_dataset_spec: RLDS_SPEC\n",
        "  episode_to_steps_fn_dataset_spec: RLDS_SPEC\n",
        "  steps_dataset_spec: Any\n",
        "  pattern: reverb.structured_writer.Pattern\n",
        "  episode_to_steps_map_fn: Any\n",
        "  expected_tensor_spec: TENSOR_SPEC\n",
        "  step_map_fn: Optional[Any] = None\n",
        "\n",
        "  def get_for_cached_trajectory_transform(self):\n",
        "    \"\"\"Creates a copy of this traj transform to use with caching.\n",
        "\n",
        "    The returned TrajectoryTransfrom copy will be initialized with the default\n",
        "    version of the `episode_to_steps_map_fn`, because the effect of that\n",
        "    function has already been materialized in the cached copy of the dataset.\n",
        "    Returns:\n",
        "      trajectory_transform: A copy of the TrajectoryTransform with overridden\n",
        "        `episode_to_steps_map_fn`.\n",
        "    \"\"\"\n",
        "    traj_copy = dataclasses.replace(self)\n",
        "    traj_copy.episode_dataset_spec = traj_copy.episode_to_steps_fn_dataset_spec\n",
        "    traj_copy.episode_to_steps_map_fn = lambda e: e[rlds_types.STEPS]\n",
        "    return traj_copy\n",
        "\n",
        "  def transform_episodic_rlds_dataset(self, episodes_dataset: tf.data.Dataset):\n",
        "    \"\"\"Applies this TrajectoryTransform to the dataset of episodes.\"\"\"\n",
        "\n",
        "    # Convert the dataset of episodes to the dataset of steps.\n",
        "    steps_dataset = episodes_dataset.map(\n",
        "        self.episode_to_steps_map_fn, num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).flat_map(lambda x: x)\n",
        "\n",
        "    return self._create_pattern_dataset(steps_dataset)\n",
        "\n",
        "  def transform_steps_rlds_dataset(\n",
        "      self, steps_dataset: tf.data.Dataset\n",
        "  ) -> tf.data.Dataset:\n",
        "    \"\"\"Applies this TrajectoryTransform to the dataset of episode steps.\"\"\"\n",
        "\n",
        "    return self._create_pattern_dataset(steps_dataset)\n",
        "\n",
        "  def create_test_dataset(\n",
        "      self,\n",
        "  ) -> tf.data.Dataset:\n",
        "    \"\"\"Creates a test dataset of trajectories.\n",
        "\n",
        "    It is guaranteed that the structure of this dataset will be the same as\n",
        "    when flowing real data. Hence this is a useful construct for tests or\n",
        "    initialization of JAX models.\n",
        "    Returns:\n",
        "      dataset: A test dataset made of zeros structurally identical to the\n",
        "        target dataset of trajectories.\n",
        "    \"\"\"\n",
        "    zeros = transformations.zeros_from_spec(self.expected_tensor_spec)\n",
        "\n",
        "    return tf.data.Dataset.from_tensors(zeros)\n",
        "\n",
        "  def _create_pattern_dataset(\n",
        "      self, steps_dataset: tf.data.Dataset) -> tf.data.Dataset:\n",
        "    \"\"\"Create PatternDataset from the `steps_dataset`.\"\"\"\n",
        "    config = create_structured_writer_config('temp', self.pattern)\n",
        "\n",
        "    # Further transform each step if the `step_map_fn` is provided.\n",
        "    if self.step_map_fn:\n",
        "      steps_dataset = steps_dataset.map(self.step_map_fn)\n",
        "    pattern_dataset = reverb.PatternDataset(\n",
        "        input_dataset=steps_dataset,\n",
        "        configs=[config],\n",
        "        respect_episode_boundaries=True,\n",
        "        is_end_of_episode=lambda x: x[rlds_types.IS_LAST])\n",
        "    return pattern_dataset\n",
        "\n",
        "\n",
        "class TrajectoryTransformBuilder(object):\n",
        "  \"\"\"Facilitates creation of the `TrajectoryTransform`.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               dataset_spec: RLDS_SPEC,\n",
        "               episode_to_steps_map_fn=lambda e: e[rlds_types.STEPS],\n",
        "               step_map_fn=None,\n",
        "               pattern_fn=None,\n",
        "               expected_tensor_spec=None):\n",
        "    self._rds_dataset_spec = dataset_spec\n",
        "    self._steps_spec = None\n",
        "    self._episode_to_steps_map_fn = episode_to_steps_map_fn\n",
        "    self._step_map_fn = step_map_fn\n",
        "    self._pattern_fn = pattern_fn\n",
        "    self._expected_tensor_spec = expected_tensor_spec\n",
        "\n",
        "  def build(self,\n",
        "            validate_expected_tensor_spec: bool = True) -> TrajectoryTransform:\n",
        "    \"\"\"Creates `TrajectoryTransform` from a `TrajectoryTransformBuilder`.\"\"\"\n",
        "\n",
        "    if validate_expected_tensor_spec and self._expected_tensor_spec is None:\n",
        "      raise ValueError('`expected_tensor_spec` must be set.')\n",
        "\n",
        "    episode_ds = zero_episode_dataset_from_spec(self._rds_dataset_spec)\n",
        "\n",
        "    steps_ds = episode_ds.flat_map(self._episode_to_steps_map_fn)\n",
        "\n",
        "    episode_to_steps_fn_dataset_spec = self._rds_dataset_spec\n",
        "\n",
        "    if self._step_map_fn is not None:\n",
        "      steps_ds = steps_ds.map(self._step_map_fn)\n",
        "\n",
        "    zeros_spec = transformations.zeros_from_spec(steps_ds.element_spec)  # pytype: disable=wrong-arg-types\n",
        "\n",
        "    ref_step = reverb.structured_writer.create_reference_step(zeros_spec)\n",
        "\n",
        "    pattern = self._pattern_fn(ref_step)\n",
        "\n",
        "    steps_ds_spec = steps_ds.element_spec\n",
        "\n",
        "    target_tensor_structure = create_reverb_table_signature(\n",
        "        'temp_table', steps_ds_spec, pattern)\n",
        "\n",
        "    if (validate_expected_tensor_spec and\n",
        "        self._expected_tensor_spec != target_tensor_structure):\n",
        "      raise RuntimeError(\n",
        "          'The tensor spec of the TrajectoryTransform doesn\\'t '\n",
        "          'match the expected spec.\\n'\n",
        "          'Expected:\\n%s\\nActual:\\n%s\\n' %\n",
        "          (str(self._expected_tensor_spec).replace('TensorSpec',\n",
        "                                                   'tf.TensorSpec'),\n",
        "           str(target_tensor_structure).replace('TensorSpec', 'tf.TensorSpec')))\n",
        "\n",
        "    return TrajectoryTransform(\n",
        "        episode_dataset_spec=self._rds_dataset_spec,\n",
        "        episode_to_steps_fn_dataset_spec=episode_to_steps_fn_dataset_spec,\n",
        "        steps_dataset_spec=steps_ds_spec,\n",
        "        pattern=pattern,\n",
        "        episode_to_steps_map_fn=self._episode_to_steps_map_fn,\n",
        "        step_map_fn=self._step_map_fn,\n",
        "        expected_tensor_spec=target_tensor_structure)\n",
        "\n",
        "def zero_episode_dataset_from_spec(rlds_spec: RLDS_SPEC):\n",
        "  \"\"\"Creates a zero valued dataset of episodes for the given RLDS Spec.\"\"\"\n",
        "\n",
        "  def add_steps(episode, step_spec):\n",
        "    episode[rlds_types.STEPS] = transformations.zero_dataset_like(\n",
        "        tf.data.DatasetSpec(step_spec))\n",
        "    if 'fake' in episode:\n",
        "      del episode['fake']\n",
        "    return episode\n",
        "\n",
        "  episode_without_steps_spec = {\n",
        "      k: v\n",
        "      for k, v in rlds_spec.episode_tensor_spec().items()\n",
        "      if k != rlds_types.STEPS\n",
        "  }\n",
        "\n",
        "  if episode_without_steps_spec:\n",
        "    episodes_dataset = transformations.zero_dataset_like(\n",
        "        tf.data.DatasetSpec(episode_without_steps_spec))\n",
        "  else:\n",
        "    episodes_dataset = tf.data.Dataset.from_tensors({'fake': ''})\n",
        "\n",
        "  episodes_dataset_with_steps = episodes_dataset.map(\n",
        "      lambda episode: add_steps(episode, rlds_spec.step_tensor_spec()))\n",
        "  return episodes_dataset_with_steps\n",
        "\n",
        "\n",
        "def create_reverb_table_signature(table_name: str, steps_dataset_spec,\n",
        "                                  pattern: reverb.structured_writer.Pattern) -> reverb.reverb_types.SpecNest:\n",
        "  config = create_structured_writer_config(table_name, pattern)\n",
        "  reverb_table_spec = reverb.structured_writer.infer_signature(\n",
        "      [config], steps_dataset_spec)\n",
        "  return reverb_table_spec\n",
        "\n",
        "\n",
        "def create_structured_writer_config(table_name: str,\n",
        "                                    pattern: reverb.structured_writer.Pattern) -> Any:\n",
        "  config = reverb.structured_writer.create_config(\n",
        "      pattern=pattern, table=table_name, conditions=[])\n",
        "  return config\n",
        "\n",
        "def n_step_pattern_builder(n: int) -> Any:\n",
        "  \"\"\"Creates trajectory of length `n` from all fields of a `ref_step`.\"\"\"\n",
        "\n",
        "  def transform_fn(ref_step):\n",
        "    traj = {}\n",
        "    for key in ref_step:\n",
        "      if isinstance(ref_step[key], dict):\n",
        "        transformed_entry = tree.map_structure(lambda ref_node: ref_node[-n:],\n",
        "                                               ref_step[key])\n",
        "        traj[key] = transformed_entry\n",
        "      else:\n",
        "        traj[key] = ref_step[key][-n:]\n",
        "\n",
        "    return traj\n",
        "\n",
        "  return transform_fn\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK4RRYkbLN5B"
      },
      "source": [
        "## Demonstration of transformation from an episode to a trajectory\n",
        "\n",
        "A real ML pipeline would rarely learn from a whole episode. Instead the input to a model is a _trajectory_. A `Trajectory` is a particular way to slice a sequence of episode steps. `SARSA` trajectory is one well known example, but a trajectory of an arbitrary length `n` is also an option. Often, a set of _overlapping_ trajectories is produced from an episode. For example, given the following episode steps:\n",
        "\n",
        "`episode=[s_0, s_1, s_2, s_3, s_4, s_T]`\n",
        "\n",
        "and a target Trajectory of length `3`, the following trajectories are produced:\n",
        "\n",
        "`t_1=[s_0, s_1, s_2]`\n",
        "\n",
        "`t_2=[s_1, s_2, s_3]`\n",
        "\n",
        "`t_3=[s_2, s_3, s_4]`\n",
        "\n",
        "`t_4=[s_3, s_4, s_T]`\n",
        "\n",
        "\n",
        "To perform such a slicing, the dataset of episode is first \"flattened\" to the dataset of steps. The `is_last` attribute of an RLDS step allows proper slicing, not crossing the episode boundary. The `TrajectoryTransformBuilder` demonstrates this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NsYnqnpNgNl"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "mt_opt_builder = tfds.builder_from_directory(builder_dir='gs://gresearch/robotics/mt_opt_rlds/1.0.0/')\n",
        "\n",
        "mt_opt_episodic_dataset = mt_opt_builder.as_dataset(split='train[:10]')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qvMcpGDx6hJ"
      },
      "outputs": [],
      "source": [
        "# The RLDSSpec for the `mt_opt` dataset.\n",
        "mt_opt_rlds_spec = RLDSSpec(\n",
        "    observation_info=b.info.features['steps']['observation'],\n",
        "    action_info=b.info.features['steps']['action'],\n",
        ")\n",
        "\n",
        "def mt_opt_step_map_fn(step):\n",
        "  transformed_step = {}\n",
        "  transformed_step['observation'] = tf.cast(tf.image.resize(step['observation']['image'], [240, 320]), tf.uint8)  # Resize to be compatible with robo_net trajectory\n",
        "  transformed_step['is_first'] = step['is_first']\n",
        "  transformed_step['is_last'] = step['is_last']\n",
        "  transformed_step['is_terminal'] = step['is_terminal']\n",
        "  return transformed_step\n",
        "\n",
        "# The following will create a trajectories of length 3.\n",
        "trajectory_length = 3\n",
        "trajectory_transform = TrajectoryTransformBuilder(mt_opt_rlds_spec, step_map_fn=mt_opt_step_map_fn, pattern_fn=n_step_pattern_builder(trajectory_length)).build(validate_expected_tensor_spec=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fk4ZfC_bMBw3"
      },
      "outputs": [],
      "source": [
        "trajectory_dataset = trajectory_transform.transform_episodic_rlds_dataset(mt_opt_episodic_dataset)\n",
        "\n",
        "trajectory_iter = iter(trajectory_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSxk3zF_x0FS"
      },
      "outputs": [],
      "source": [
        "trajectory = next(trajectory_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2V0xrIVMWNc"
      },
      "outputs": [],
      "source": [
        "trajectory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytrvi945NTZz"
      },
      "outputs": [],
      "source": [
        "# Note that the leading dimension (3) corresponds to the trajectory_length\n",
        "trajectory['observation'].shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhDX3BcWNmrl"
      },
      "outputs": [],
      "source": [
        "images = [trajectory['observation'][id] for id in range(trajectory['observation'].shape[0])]\n",
        "images = [Image.fromarray(image.numpy()) for image in images]\n",
        "\n",
        "display.Image(as_gif(images))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy89HzymQyAq"
      },
      "source": [
        "## Combination of multiple datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qs0-7alaQ3C9"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "robo_net_builder = tfds.builder_from_directory(builder_dir='gs://gresearch/robotics/robo_net/1.0.0/')\n",
        "\n",
        "robo_net_builder_episodic_dataset = robo_net_builder.as_dataset(split='train[:10]')\n",
        "episodes = list(iter(robo_net_builder_episodic_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tgJpMqARIFQ"
      },
      "outputs": [],
      "source": [
        "# The following will create a trajectories of length 3.\n",
        "trajectory_length = 3\n",
        "\n",
        "robo_net_rlds_spec = RLDSSpec(\n",
        "    observation_info=robo_net_builder.info.features['steps']['observation'],\n",
        "    action_info=robo_net_builder.info.features['steps']['action'],\n",
        ")\n",
        "\n",
        "def robo_net_step_map_fn(step):\n",
        "  transformed_step = {}\n",
        "  transformed_step['observation'] = step['observation']['image']\n",
        "  transformed_step['is_first'] = step['is_first']\n",
        "  transformed_step['is_last'] = step['is_last']\n",
        "  transformed_step['is_terminal'] = step['is_terminal']\n",
        "  return transformed_step\n",
        "\n",
        "robo_net_trajectory_transform = TrajectoryTransformBuilder(robo_net_rlds_spec,\n",
        "                                                                          step_map_fn=robo_net_step_map_fn,\n",
        "                                                                          pattern_fn=n_step_pattern_builder(trajectory_length)).build(validate_expected_tensor_spec=False)\n",
        "\n",
        "\n",
        "def mt_opt_step_map_fn(step):\n",
        "  transformed_step = {}\n",
        "  transformed_step['observation'] = tf.cast(tf.image.resize(step['observation']['image'], [240, 320]), tf.uint8)  # Resize to be compatible with robo_net trajectory\n",
        "  transformed_step['is_first'] = step['is_first']\n",
        "  transformed_step['is_last'] = step['is_last']\n",
        "  transformed_step['is_terminal'] = step['is_terminal']\n",
        "  return transformed_step\n",
        "\n",
        "mt_opt_trajectory_transform = TrajectoryTransformBuilder(mt_opt_rlds_spec,\n",
        "                                                         step_map_fn=mt_opt_step_map_fn,\n",
        "                                                         pattern_fn=n_step_pattern_builder(trajectory_length)).build(validate_expected_tensor_spec=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anGArTQbTiHj"
      },
      "outputs": [],
      "source": [
        "# Validate that the specs are equal\n",
        "assert robo_net_trajectory_transform.expected_tensor_spec == mt_opt_trajectory_transform.expected_tensor_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9gRx6BfTGH-"
      },
      "outputs": [],
      "source": [
        "# Create trajectory datasets for the two normalized representations:\n",
        "robo_net_trajectory_dataset = robo_net_trajectory_transform.transform_episodic_rlds_dataset(robo_net_builder_episodic_dataset)\n",
        "mt_opt_trajectory_dataset = mt_opt_trajectory_transform.transform_episodic_rlds_dataset(mt_opt_episodic_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SVkHpIxRVXz"
      },
      "outputs": [],
      "source": [
        "combined_dataset = tf.data.Dataset.sample_from_datasets([robo_net_trajectory_dataset, mt_opt_trajectory_dataset])\n",
        "combined_dataset = combined_dataset.batch(2)\n",
        "combined_dataset_it = iter(combined_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CMdwIcsR30k"
      },
      "outputs": [],
      "source": [
        "example = next(combined_dataset_it)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2YJOvRKUb2E"
      },
      "outputs": [],
      "source": [
        "# First element of the batch returns a robot_net trajectory\n",
        "Image.fromarray(example['observation'].numpy()[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FP0iz-f_UoTY"
      },
      "outputs": [],
      "source": [
        "# Second element of the batch returns a mt_opt trajectory\n",
        "Image.fromarray(example['observation'].numpy()[1][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2Efw2aHVfSX"
      },
      "source": [
        "# Available datasets and their sizes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQkeUKyrVhGK"
      },
      "outputs": [],
      "source": [
        "# Iterate over and make sure that a dataset can be created\n",
        "for name in DATASETS:\n",
        "  uri = dataset2path(name)\n",
        "  b = tfds.builder_from_directory(builder_dir=uri)\n",
        "  split = list(b.info.splits.keys())[0]\n",
        "  b.as_dataset(split=split)\n",
        "  print('Dataset %s has size %s'%(uri, b.info.dataset_size))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPhwnlk1a1lq"
      },
      "outputs": [],
      "source": [
        "from tensorflow_datasets.core.registered import DatasetNotFoundError\n",
        "\n",
        "for name in DATASET_NAMES:\n",
        "  try:\n",
        "    b = tfds.builder(name)\n",
        "    print(f\"Dataset {name} is found in the TFDS catalog.\")\n",
        "  except DatasetNotFoundError:\n",
        "    print(f\"\\033[91mDataset {name} is NOT found in the TFDS catalog.\\033[0m\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}